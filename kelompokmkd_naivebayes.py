# -*- coding: utf-8 -*-
"""KelompokMKD_NaiveBayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ivbDtIpgUrROmCd3XHv3WfG_so87tnsT
"""

# import modul yang dibutuhkan
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt # modul untuk membuat diagram batang

"""# **Read Data**"""

# masukkan URL ke variable 'file_url'
# Data tersebut merupakan data 'arrhythmia.data'
file_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data'

# baca dengan pd.read_csv() dengan argumen header=None(menandakan tidak ada baris header) dan na_values='?'(semua nilai '?' diubah menjadi NaN)
dataf = pd.read_csv(file_url, header=None, na_values='?')

# Rename semua column sesuai 'arrhythmia.names'
dataf.set_axis(["Age", "Sex", "Height", "Weight", "QRS Duration", "P-R Interval", "Q-T Interval", "T Interval", "P Interval", "QRS Vector_A", "T Vector_A", "P Vector_A", "QRST Vector_A",
               "J Vector_A", "Heart Rate","Q Wave_W_DI", "R Wave_W_DI", "S Wave_W_DI", "R` Wave_W_DI", "S` Wave_W_DI", "Intrinsic_Deflect_DI", "R_R Wave DI", "D_D_R Wave DI", "R_P Wave DI",
               "D_D_P Wave DI", "R_T Wave DI", "D_D_T Wave DI","Q Wave_W_DII", "R Wave_W_DII", "S Wave_W_DII", "R` Wave_W_DII", "S` Wave_W_DII", "Intrinsic_Deflect_DII", "R_R Wave DII", "D_D_R Wave DII",
               "R_P Wave DII", "D_D_P Wave DII", "R_T Wave DII", "D_D_T Wave DII","Q Wave_W_DIII", "R Wave_W_DIII", "S Wave_W_DIII", "R` Wave_W_DIII", "S` Wave_W_DIII", "Intrinsic_Deflect_DIII",
               "R_R Wave DIII", "D_D_R Wave DIII", "R_P Wave DIII", "D_D_P Wave DIII", "R_T Wave DIII", "D_D_T Wave DIII","Q Wave_W_AVR", "R Wave_W_AVR", "S Wave_W_AVR", "R` Wave_W_AVR", "S` Wave_W_AVR",
               "Intrinsic_Deflect_AVR", "R_R Wave AVR", "D_D_R Wave AVR", "R_P Wave AVR", "D_D_P Wave AVR", "R_T Wave AVR", "D_D_T Wave AVR","Q Wave_W_AVL", "R Wave_W_AVL", "S Wave_W_AVL", "R` Wave_W_AVL",
               "S` Wave_W_AVL", "Intrinsic_Deflect_AVL", "R_R Wave AVL", "D_D_R Wave AVL", "R_P Wave AVL", "D_D_P Wave AVL", "R_T Wave AVL", "D_D_T Wave AVL","Q Wave_W_AVF", "R Wave_W_AVF", "S Wave_W_AVF",
               "R` Wave_W_AVF", "S` Wave_W_AVF", "Intrinsic_Deflect_AVF", "R_R Wave AVF", "D_D_R Wave AVF", "R_P Wave AVF", "D_D_P Wave AVF", "R_T Wave AVF", "D_D_T Wave AVF","Q Wave_W_V1", "R Wave_W_V1",
               "S Wave_W_V1", "R` Wave_W_V1", "S` Wave_W_V1", "Intrinsic_Deflect_V1", "R_R Wave V1", "D_D_R Wave V1", "R_P Wave V1", "D_D_P Wave V1", "R_T Wave V1", "D_D_T Wave V1","Q Wave_W_V2", "R Wave_W_V2",
               "S Wave_W_V2", "R` Wave_W_V2", "S` Wave_W_V2", "Intrinsic_Deflect_V2", "R_R Wave V2", "D_D_R Wave V2", "R_P Wave V2", "D_D_P Wave V2", "R_T Wave V2", "D_D_T Wave V2","Q Wave_W_V3", "R Wave_W_V3",
               "S Wave_W_V3", "R` Wave_W_V3", "S` Wave_W_V3", "Intrinsic_Deflect_V3", "R_R Wave V3", "D_D_R Wave V3", "R_P Wave V3", "D_D_P Wave V3", "R_T Wave V3", "D_D_T Wave V3","Q Wave_W_V4", "R Wave_W_V4",
               "S Wave_W_V4", "R` Wave_W_V4", "S` Wave_W_V4", "Intrinsic_Deflect_V4", "R_R Wave V4", "D_D_R Wave V4", "R_P Wave V4", "D_D_P Wave V4", "R_T Wave V4", "D_D_T Wave V4","Q Wave_W_V5", "R Wave_W_V5",
               "S Wave_W_V5", "R` Wave_W_V5", "S` Wave_W_V5", "Intrinsic_Deflect_V5", "R_R Wave V5", "D_D_R Wave V5", "R_P Wave V5", "D_D_P Wave V5", "R_T Wave V5", "D_D_T Wave V5","Q Wave_W_V6", "R Wave_W_V6",
               "S Wave_W_V6", "R` Wave_W_V6", "S` Wave_W_V6", "Intrinsic_Deflect_V6", "R_R Wave V6", "D_D_R Wave V6", "R_P Wave V6", "D_D_P Wave V6", "R_T Wave V6", "D_D_T Wave V6","JJ Wave_Amp_DI", "Q Wave_Amp_DI",
               "R Wave_Amp_DI", "S Wave_Amp_DI", "R` Wave_Amp_DI", "S` Wave_Amp_DI", "P Wave_Amp_DI", "T Wave_Amp_DI", "QRSA DI", "QRSTA DI","JJ Wave_Amp_DII", "Q Wave_Amp_DII", "R Wave_Amp_DII", "S Wave_Amp_DII",
               "R` Wave_Amp_DII", "S` Wave_Amp_DII", "P Wave_Amp_DII", "T Wave_Amp_DII", "QRSA DII", "QRSTA DII","JJ Wave_Amp_DIII", "Q Wave_Amp_DIII", "R Wave_Amp_DIII", "S Wave_Amp_DIII", "R` Wave_Amp_DIII",
               "S` Wave_Amp_DIII", "P Wave_Amp_DIII", "T Wave_Amp_DIII", "QRSA DIII", "QRSTA DIII","JJ Wave_Amp_AVR", "Q Wave_Amp_AVR", "R Wave_Amp_AVR", "S Wave_Amp_AVR", "R` Wave_Amp_AVR", "S` Wave_Amp_AVR",
               "P Wave_Amp_AVR", "T Wave_Amp_AVR", "QRSA AVR", "QRSTA AVR","JJ Wave_Amp_AVL", "Q Wave_Amp_AVL", "R Wave_Amp_AVL", "S Wave_Amp_AVL", "R` Wave_Amp_AVL", "S` Wave_Amp_AVL", "P Wave_Amp_AVL",
               "T Wave_Amp_AVL", "QRSA AVL", "QRSTA AVL","JJ Wave_Amp_AVF", "Q Wave_Amp_AVF", "R Wave_Amp_AVF", "S Wave_Amp_AVF", "R` Wave_Amp_AVF", "S` Wave_Amp_AVF", "P Wave_Amp_AVF", "T Wave_Amp_AVF", "QRSA AVF",
               "QRSTA AVF","JJ Wave_Amp_V1", "Q Wave_Amp_V1", "R Wave_Amp_V1", "S Wave_Amp_V1", "R` Wave_Amp_V1", "S` Wave_Amp_V1", "P Wave_Amp_V1", "T Wave_Amp_V1", "QRSA V1", "QRSTA V1","JJ Wave_Amp_V2",
               "Q Wave_Amp_V2", "R Wave_Amp_V2", "S Wave_Amp_V2", "R` Wave_Amp_V2", "S` Wave_Amp_V2", "P Wave_Amp_V2", "T Wave_Amp_V2", "QRSA V2", "QRSTA V2","JJ Wave_Amp_V3", "Q Wave_Amp_V3", "R Wave_Amp_V3",
               "S Wave_Amp_V3", "R` Wave_Amp_V3", "S` Wave_Amp_V3", "P Wave_Amp_V3", "T Wave_Amp_V3", "QRSA V3", "QRSTA V3","JJ Wave_Amp_V4", "Q Wave_Amp_V4", "R Wave_Amp_V4", "S Wave_Amp_V4", "R` Wave_Amp_V4",
               "S` Wave_Amp_V4", "P Wave_Amp_V4", "T Wave_Amp_V4", "QRSA V4", "QRSTA V4","JJ Wave_Amp_V5", "Q Wave_Amp_V5", "R Wave_Amp_V5", "S Wave_Amp_V5", "R` Wave_Amp_V5", "S` Wave_Amp_V5", "P Wave_Amp_V5",
               "T Wave_Amp_V5", "QRSA V5", "QRSTA V5","JJ Wave_Amp_V6", "Q Wave_Amp_V6", "R Wave_Amp_V6", "S Wave_Amp_V6", "R` Wave_Amp_V6", "S` Wave_Amp_V6", "P Wave_Amp_V6", "T Wave_Amp_V6", "QRSA V6", "QRSTA V6",
               "Class"], axis = "columns", inplace =True)

"""# **Output Isi Data**"""

# output isi dataframe dataf
dataf

# output 5 baris pertama dataf
dataf.head()

# output 5 baris terakhir dataf
dataf.tail()

# output informasi dataf termasuk jumlah baris dan kolom, tipe data kolom
dataf.info()

# output informasi statisk dataf termasuk count, mean, std, min, quartil, dan max
dataf.describe()

"""# **Data Preprocessing**

## **Handle missing values**
"""

# function untuk check missing values
def check_missing_values(df):
    missing_values = {}
    total_rows = len(df)

    for column in df.columns:
        missing_count = df[column].isna().sum()
        if missing_count > 0:
            missing_values[column] = missing_count

    # Jika tidak ada missing values pada datagram frame akan keluar dari function
    if len(missing_values) == 0:
        print(f'Tidak ada missing values pada Data Frame ini')
        return

    for column, count in missing_values.items():
        percentage = (count / total_rows) * 100
        print(f"Column '{column}': {count} missing value(s) ({percentage:.2f}%)")

    # buat bar chart
    if len(missing_values) > 0:
        # buat list columns, count missing value(s), dan persen
        columns = list(missing_values.keys())
        counts = list(missing_values.values())
        percentages = [(count / total_rows) * 100 for count in counts]

        # Plot missing value(s) menggunakan bar chart
        fig, ax = plt.subplots()
        ax.bar(columns, counts)

        # tambah label persen
        for i, count in enumerate(counts):
            ax.text(i, count, f"{percentages[i]:.2f}%", ha='center', va='bottom')

        # Set labels dan titles
        ax.set_xlabel('Columns')
        ax.set_ylabel('Missing Value Count')
        ax.set_title('Missing Values')
        ax.set_xticklabels(columns, rotation=90)

        plt.tight_layout()
        plt.show()

# panggil procedure
check_missing_values(dataf)

"""Dataframe dengan missing value yang lebih dari 80% akan di drop dimana hanya column 'J Vector_A' dengan 376 missing values akan di drop. Untuk column lain dengan data frame yang ada missing value(s) akan diisi dengan nilai mean berdasarkan columnnya."""

# Drop column 'J Vector_A'
dataf = dataf.drop('J Vector_A', axis=1)
dataf

# isi sisa missing values yang ada di dataframe dengan mean values tiap column
mean_values = dataf.mean()
dataf = dataf.fillna(mean_values)
dataf

# Cek ulang missing values
check_missing_values(dataf)

"""## **Handle Data bernilai nol**



"""

# function hapus column yang data bernilai nol lebih dari threshold
def drop_columns_with_zeros(data, threshold):
    total_rows = len(data)
    zero_values = data[data == 0].count()
    zero_percentages = zero_values / total_rows * 100

    columns_to_drop = zero_percentages[zero_percentages > threshold].index
    data = data.drop(columns_to_drop, axis=1)

    dropped_columns = pd.DataFrame({'Column': columns_to_drop, 'Zero Percentage': zero_percentages[columns_to_drop]})

    print("Dropped Columns:\n")
    print(f"Columns\t\tZero Percantage")
    for _, row in dropped_columns.iterrows():
        print(f"{row['Column']}\t{row['Zero Percentage']:.2f}%")

    return data

# function hapus rows yang data bernilai nol lebih dari threshold
def drop_rows_with_zeros(data, threshold):
    total_columns = len(data.columns)
    zero_counts = (data == 0).sum(axis=1)
    zero_percentages = zero_counts / total_columns * 100

    rows_to_drop = zero_percentages[zero_percentages > threshold].index
    dropped_rows = data.loc[rows_to_drop]
    data = data.drop(rows_to_drop)

    print("Dropped Rows:\n")
    print(f"Index\t\tZero Percentage")
    for idx, row in dropped_rows.iterrows():
        print(f"{idx}\t\t{zero_percentages.loc[idx]:.2f}%")

    return data

# set dulu supaya column sex tidak dihapus (karena dijelaskan di 'arrhythmia.names' bahwa sex 0 = male dan 1 = female, jdi tidak boleh dihapus)
dataf['Sex'] = dataf['Sex'].replace(0, 2)

# panggil function hapus column dengan threshold 50%
dataf = drop_columns_with_zeros(dataf, 50)

"""148 dari 279 column dihapus karena memiliki lebih dari 50% data yang bernilai nol."""

# panggil function hapus baris dengan threshold 30%
dataf = drop_rows_with_zeros(dataf, 30)

"""1 dari 452 column dihapus karena memiliki lebih dari 30% data yang bernilai nol."""

# set balik column sex
dataf['Sex'] = dataf['Sex'].replace(2, 0)

# cek hasil data frame setelah data processing
dataf

"""# **Output data setelah preprocessing**"""

# output informasi dataf termasuk jumlah baris dan kolom, tipe data kolom
dataf.info()

# output informasi statisk dataf termasuk count, mean, std, min, quartil, dan max
dataf.describe()

"""# **Setting Variable**"""

# membuat variable untuk store semua column , dan store semua column kecuali class
Cdataf = dataf.drop('Class', axis=1)
columnss = list(Cdataf.columns.values)

ccolumns = list(dataf.columns.values)

# Set yang class = 1 untuk normal dan class[2..16] untuk sakit
dataf['Class'] = dataf['Class'].apply(lambda x: 0 if x != 1 else x)

"""# **Metode Naive Bayes**"""

# Mencari normalisasi data dengan min-max normalization
def normalizationMinMax(Data, columnTarget):
    for column in columnTarget:
        min_val = Data[column].min()
        max_val = Data[column].max()

        if min_val != max_val:
            Data[column] = (Data[column] - min_val) / (max_val - min_val)

    return Data

# panggil function normalizationMinMax
normalizationData = dataf.copy()
normDataf = normalizationMinMax(normalizationData, columnss)
print(normDataf)

# Mencari standardization data
def standardization(Data, columnTarget):
  for column in columnTarget:
        min_val = Data[column].min()
        max_val = Data[column].max()

        if min_val != max_val:
            Data[column] = (Data[column]-Data[column].mean())/(Data[column].std())
  return Data

# panggil function standardization
standardizedData = dataf.copy()
stdDataf = standardization(standardizedData, columnss)
print(stdDataf)

# split data menjadi class normal dan class sick
def separate_by_class(dataf):
    separated = {}
    separated['normal'] = dataf[dataf['Class'] == 1]
    separated['sick'] = dataf[dataf['Class'] != 1]

    return separated

# panggil fungsi
separated_data = separate_by_class(dataf)

# Print isi class normal dan class sick
class_normal = separated_data['normal']
class_sick = separated_data['sick']
print("Class Normal: ")
print(class_normal)
print(f"\nClass Sick:")
print(class_sick)

# mencari mean setiap column
def calculate_column_means(class_normal, class_sick, columnTarget):
  normalMean = {}
  sickMean = {}
  for column in columnTarget:
    normalMean[column] = class_normal[column].mean()
    sickMean[column] = class_sick[column].mean()
  return normalMean, sickMean

# print mean setiap column untuk class normal dan class sick
df_normalMean, df_sickMean = calculate_column_means(class_normal, class_sick, ccolumns)
print(f'Mean Result\nClass Normal : {df_normalMean}\nClass Sick : {df_sickMean}')

# mencari std setiap column
def calculate_column_std(class_normal, class_sick, columnTarget):
  normalStd = {}
  sickStd = {}
  for column in columnTarget:
    normalStd[column] = class_normal[column].std()
    sickStd[column] = class_sick[column].std()
  return normalStd, sickStd

# print std setiap column untuk class normal dan class sick
df_normalStd, df_sickStd = calculate_column_std(class_normal, class_sick, ccolumns)
print(f'STDs Result\nClass Normal : {df_normalStd}\nClass Sick : {df_sickStd}')

# function untuk cari Gaussian Probability Density
def calc_probability(mean, std, x):
  std +=1
  exponent = math.exp(-((x-mean)**2/(2*std**2)))
  return (1/(math.sqrt(2*math.pi)*std))*exponent

# procedure untuk menghitung evaluasi dan melakukan visualisasi dari hasil prediksi
def confussionMatrix(result):
    TP = 0
    FP = 0
    TN = 0
    FN = 0

    x = True

    for i in result:
        # cek dulu apakah ada Ground truth bernilai ('?'), akan break jika ada
        if i['Ground Truth'] == '?':
            x = False
            break
        # jika prediksi dan ground truth sama dengan prediction result, maka TP akan bertambah satu
        elif i['Prediction Result'] == 1 and i['Prediction Result'] == i['Ground Truth']:
            TP += 1
        # jika prediksi dan ground truth sama dengan prediction result, maka TN akan bertambah satu
        elif i['Prediction Result'] == 0 and i['Prediction Result'] == i['Ground Truth']:
            TN += 1
        # jika prediksi bernilai 1 tetapi ground truth berbeda dengan prediction result, maka FP akan bertambah satu
        elif i['Prediction Result'] == 1 and i['Prediction Result'] != i['Ground Truth']:
            FP += 1
        # jika prediksi bernilai 0 tetapi ground truth berbeda dengan prediction result, maka FN akan bertambah satu
        elif i['Prediction Result'] == 0 and i['Prediction Result'] != i['Ground Truth']:
            FN += 1

    # jika x bernilai true akan output TP, TN, FP, FN, akurasi, presisi, dan recall, beserta visualisasi
    if x:
        akurasi = ((TP+TN)/(TP+TN+FP+FN))*100
        presisi = ((TP)/(TP+FP))*100
        recall = ((TP)/(TP+FN))*100

        print(f"\nTP : {TP}  FN : {FP}\nTN : {TN}  FN : {FN}")
        print(f"Accuracy : {akurasi}%")
        print(f"Precission : {presisi}%")
        print(f"Recall : {recall}%")

        # visualisasi confusion matrix
        labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']
        values = [TN, FP, FN, TP]

        plt.bar(labels, values)
        plt.title('Confusion Matrix')
        plt.xlabel('Prediction')
        plt.ylabel('Count')
        plt.text(3.8, 0, f"Accuracy : {akurasi}%\nPrecission : {presisi}%\nRecall : {recall}%")
        plt.show()

    # jika x bernilai false akan output tidak bisa proses confusion matrix
    else:
        print("\nCannot process the confusion matrix")

# fungsi untuk melakukan prediksi kelas berdasarkan model yang telah ditraining
def doPrediction(normalMean, normalStd, sickMean, sickStd, target, columnTarget, truthColumn):
  result = []
  # iterate sebanyak jumlah baris target
  for i in range(len(target)):

    # setiap iterasi akan set normalResult dan sickResult menjadi 1 untuk hasil perhitungan probabilitas masing-masing kelas.
    normalResult = 1
    sickResult = 1

    # iterate sebanyak jumlah kolom columnTarget
    for column in columnTarget:

      # untuk setiap kolom akan dikali normal result dengan hasil calc_probability normal dan dikali juga sick result dengan hasil calc_probability sick
      normalResult *= calc_probability(normalMean[column], normalStd[column], target[column].iloc[i])
      sickResult *= calc_probability(sickMean[column], sickStd[column], target[column].iloc[i])

    # hasilnya akan dimasukkan ke result
    result.append({'ID' : i, 'Normal Probability' : "{}".format(normalResult), 'Sick Probability' : "{}".format(sickResult),
                        'Prediction Result' : int(normalResult > sickResult),
                       'Ground Truth' : target[truthColumn].iloc[i]})
  return result

# fungsi untuk membagi dataset menjadi fold untuk train data
def folding(dataset, trainingPercentage, location, shuffle:bool):
    # menghitung panjang training berdasarkan persentase training yang diberikan
    lengthTraining = int(len(dataset)*trainingPercentage/100)

    # jika shuffle itu true maka akan di shuffle dulu datasetnya
    if(shuffle):
        dataset = dataset.sample(frac=1).reset_index(drop=True)

    # inisiasi variable subset train dan validation sebagai list kosong
    train = []
    validation = []

    # jika location left, train berisi awal hingga lengthTraining lalu validation dari lengthTraing sampai habis
    if(location == 'left'):
        train, validation = dataset.iloc[:lengthTraining].reset_index(drop=True), dataset.iloc[lengthTraining:].reset_index(drop=True)

    # jika location right, train berisi awal hingga panjang yang dihitung berdasarkan perbedaan antara lengthTraining dan panjang dataset lalu validation dari panjang tersebut sampai habis
    elif(location == 'right'):
        validation,train = dataset.iloc[:abs(lengthTraining-len(dataset))].reset_index(drop=True), dataset.iloc[abs(lengthTraining-len(dataset)):].reset_index(drop=True)

    # jika location middle, train berisi data tengah dikurangi setengah perbedaan panjang lalu validation berisi gabungan data sebelum dan setelah setengah perbedaan panjang dataset
    elif(location == 'middle'):
        train = dataset.iloc[int(abs(lengthTraining-len(dataset))/2):len(dataset)-int(abs(lengthTraining-len(dataset))/2)]
        validation = pd.concat([dataset.iloc[:int(abs(lengthTraining-len(dataset))/2)],dataset.iloc[len(dataset)-int(abs(lengthTraining-len(dataset))/2):]])
    return train, validation

# panggil fungsi folding
trainData, validationData = folding(dataf.copy(), 70, 'left',shuffle=True)

# output isi trainData
trainData

# output isi validationData
validationData

# train data di split berdasarkan class
separate_traindata = separate_by_class(dataf)

# print isi train_class_normal dan train_class_sick
train_class_normal = separate_traindata['normal']
train_class_sick = separate_traindata['sick']
print("Class Normal: ")
print(train_class_normal)
print(f"\nClass Sick:")
print(train_class_sick)

# calculate mean setiap column untuk train_class_normal dan train_class_sick lalu di print
train_normalMean, train_sickMean = calculate_column_means(train_class_normal, train_class_sick, ccolumns)
print(f'Mean Result\nClass Normal : {train_normalMean}\nClass Sick : {train_sickMean}')

# calculate std setiap column untuk train_class_normal dan train_class_sick lalu di print
train_normalStd, train_sickStd = calculate_column_std(train_class_normal, train_class_sick, ccolumns)
print(f'STDs Result\nClass Normal : {train_normalStd}\nClass Sick : {train_sickStd}')

# result
# panggil function doPrediction
result = []

target = validationData # your ground truth data
result = doPrediction(train_normalMean, train_normalStd, train_sickMean, train_sickStd, target, ccolumns, truthColumn='Class')

# Print prediksi dan panggil procedure confussionMatrix
for p in result:
    print(p)
confussionMatrix(result)